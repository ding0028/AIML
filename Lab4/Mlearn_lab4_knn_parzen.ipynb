{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5OGyRdOAR2M0"},"source":["# **Machine Learning from Data**\n","\n","## Lab 4: K-Nearest Neighbors and Parzen windows\n","\n","2021 - Veronica Vilaplana - [GPI @ IDEAI](https://imatge.upc.edu/web/) Research group\n","\n","-----------------\n"]},{"cell_type":"code","metadata":{"id":"Gu65cV0qTxH5"},"source":["import numpy as np\n","from numpy.linalg import pinv\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.decomposition import PCA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neighbors import KernelDensity\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","\n","from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\n","from sklearn.base import ClassifierMixin, BaseEstimator\n","from sklearn.utils.multiclass import unique_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YiqW-GZSQh5"},"source":["#K-nearest neighbor classifier\n","\n","The kNN classifier is a type of instance-based or non-generalizing learning, in the sense that it does not try to build a general model but simply stores training samples. Classification is done by majority voting of the nearest neighbors of each sample. A test sample is assigned to the class which has the most representatives within the k nearest neighbors of the sample.\n","\n","We will use one implementation of kNN from the **scikit-learn** library (`KNeighborsClassifier`). See https://scikit-learn.org/stable/modules/neighbors.html#classification.\n","\n","As all supervised estimators in scikit-learn, we need to create an instance of the class `KNeighborsClassifier`, use a  `fit(X, y)` method to fit the model and a `predict(X)` method that, given unlabeled observations X, returns the predicted labels y.\n","\n","The main parameters of this function are\n","- n_neighbors (default = 5): number of neighbors\n","- weights{‘uniform’, ‘distance’} or callable, default=’uniform’: weight function used in prediction.\n","- algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’:\n","algorithm used to compute the nearest neighbors"]},{"cell_type":"markdown","metadata":{"id":"EeYQ-MD_z7mb"},"source":["##Example [from sklearn](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py): kNN using majority voting\n","\n","Let's see a simple example of a kNN classifier using samples from the Iris dataset. We will use only two of the four available features per sample (Sepal Length, Sepal Width). There are three classes (Setosa, Versicolor, Virginica).\n","Change the number of neighbors to see the effect of this hyperparameter."]},{"cell_type":"code","metadata":{"id":"qfB987hAzPUJ"},"source":["from matplotlib.colors import ListedColormap\n","from sklearn import datasets\n","\n","n_neighbors = 15\n","\n","# load the iris dataset\n","iris = datasets.load_iris()\n","\n","# we only take the first two features. We could avoid this slicing by using a two-dim dataset\n","X = iris.data[:, :2]\n","y = iris.target\n","\n","h = 0.02  # step size in the mesh\n","\n","# Create color maps\n","cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n","cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n","\n","# we create an instance of Neighbours Classifier and fit the data.\n","clf = KNeighborsClassifier(n_neighbors)\n","clf.fit(X, y)\n","\n","# Plot the decision boundary. For that, we will assign a color to each\n","# point in the mesh [x_min, x_max]x[y_min, y_max].\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","\n","# Put the result into a color plot\n","Z = Z.reshape(xx.shape)\n","plt.figure(figsize=(8, 6))\n","plt.contourf(xx, yy, Z, cmap=cmap_light)\n","\n","# Plot also the training points\n","sns.scatterplot(\n","    x=X[:, 0],\n","    y=X[:, 1],\n","    hue=iris.target_names[y],\n","    palette=cmap_bold,\n","    alpha=1.0,\n","    edgecolor=\"black\",\n",")\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.title(\"3-Class classification (k = %i, weights = 'uniform' (default))\" % (n_neighbors))\n","plt.xlabel(iris.feature_names[0])\n","plt.ylabel(iris.feature_names[1])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Example [from sklearn](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py): kNN using distance-based weights\n","\n","The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the weights keyword. The default value, weights = 'uniform', assigns uniform weights to each neighbor. weights = 'distance' assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.\n","\n","Let's see the second case on the iris dataset:"],"metadata":{"id":"gNRZ2vrpplbB"}},{"cell_type":"code","source":["\n","# we create an instance of Neighbours Classifier and fit the data.\n","clf = KNeighborsClassifier(n_neighbors, weights= 'distance')\n","clf.fit(X, y)\n","\n","# Plot the decision boundary. For that, we will assign a color to each\n","# point in the mesh [x_min, x_max]x[y_min, y_max].\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","\n","# Put the result into a color plot\n","Z = Z.reshape(xx.shape)\n","plt.figure(figsize=(8, 6))\n","plt.contourf(xx, yy, Z, cmap=cmap_light)\n","\n","# Plot also the training points\n","sns.scatterplot(\n","    x=X[:, 0],\n","    y=X[:, 1],\n","    hue=iris.target_names[y],\n","    palette=cmap_bold,\n","    alpha=1.0,\n","    edgecolor=\"black\",\n",")\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.title(\"3-Class classification (k = %i, weights = 'distance')\" % (n_neighbors))\n","plt.xlabel(iris.feature_names[0])\n","plt.ylabel(iris.feature_names[1])\n","\n","plt.show()"],"metadata":{"id":"SMTNzZCuqA-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PI0ld0I2zv4Z"},"source":["# Classification of the Zip dataset"]},{"cell_type":"markdown","metadata":{"id":"Hw7lWIRDPt6y"},"source":["## Data loading\n","\n","The Zip code is a dataset of handwritten digits, scanned from envelopes by the U.S. Postal Service. The images have been deslanted and sized normalized. Each element is a vector of d=256 features corresponding to intensity values of a 16x16 image of a handwritten digit. Images are vectorized row by row. There are 10 classes (digits 0 to 9). The dataset is already split into training and test subsets.\n","\n","The training and testing splits are on separate files and consist on text files with a line for each sample, each line has the label and the pixel values separated by spaces.\n","\n","Load the files `zip_train` and `zip_test`."]},{"cell_type":"code","metadata":{"id":"g3nu5QwDJ-dh"},"source":["TRAIN_DATA_FILENAME = \"zip_train\"\n","TEST_DATA_FILENAME = \"zip_test\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_8h7v0vQXAh"},"source":["We can define a utility function to load the data from the text files and return the images and labels as separate [NumPy ndarrays](https://numpy.org/doc/stable/reference/arrays.ndarray.html)."]},{"cell_type":"code","metadata":{"id":"9NFvMskWIiKD"},"source":["def load_dataset_as_ndarray(filename, reshape=False):\n","    \"\"\"Return images and labels form filename as NumPy arrays.\n","\n","    If reshape is True, reshape samples to 16x16.\n","    \"\"\"\n","    data = np.loadtxt(filename)\n","    X = data[:, 1:]\n","    y = data[:, 0]\n","    if reshape:\n","        X = X.reshape((-1, 16, 16))\n","    return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbFIxwKTKyOt"},"source":["# Load the training split.\n","X_train, y_train = load_dataset_as_ndarray(TRAIN_DATA_FILENAME)\n","# Load the test split.\n","X_test, y_test = load_dataset_as_ndarray(TEST_DATA_FILENAME)\n","print(X_train.shape)\n","print(X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cFKAUDc9x7or"},"source":["## Training a k-NN classifier\n","Wel will train a dimensionality reduction technique for feature selection and a k-nearst neighbour classifer.\n","\n","First, we set all the variables defining the experiment to be performed: with or without feature selection (`reduction`), technique to use PCA or MDA (`dim_reducer`) and number of features (`n_components`)"]},{"cell_type":"code","metadata":{"id":"AU5nlX5vMGQC"},"source":["reduction = int(input(\"Choose reduction type.\\nNo reduction (0), PCA (1), MDA(2): \"))\n","if reduction > 0:\n","    n_components = int(input(\"Choose dimenstionality: \"))\n","    if reduction == 1:\n","        dim_reducer = PCA(n_components=n_components)\n","    elif reduction == 2:\n","        dim_reducer = LinearDiscriminantAnalysis(n_components=n_components)\n","    else:\n","        dim_reducer = None\n","elif reduction == 0:\n","    dim_reducer = \"passthrough\"\n","else:\n","    dim_reducer = None\n","\n","if not dim_reducer:\n","    print(\"Please choose one of the valid options (run this cell again).\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8TpsbHiFC28"},"source":["##Pipelines\n","\n","Scikit-learn provides a `Pipeline` utility to help automate machine learning workflows. Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated.\n","\n","A [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n","\n","**Convenience and encapsulation**\n","You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n","\n","**Joint parameter selection**\n","You can grid search over parameters of all estimators in the pipeline at once.\n","\n","**Safety**\n","Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n","\n","All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).\n"]},{"cell_type":"markdown","metadata":{"id":"8H4weWwrGZuk"},"source":["In this case we define a pipeline that\n","1. Standardizes features by removing the mean\n","2. Reduces dimensionality and\n","3. Applies a kNN classifier:\n","\n"]},{"cell_type":"code","metadata":{"id":"2TzMbEf3Pequ"},"source":["model = Pipeline([\n","    ('center', StandardScaler(with_mean=True, with_std=False)),\n","    ('reduce_dim', dim_reducer),\n","    ('clf', KNeighborsClassifier())\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SERcU_cTH4n3"},"source":["Fit all the transformers one after the other and transform the data. Finally, fit the transformed data using the final estimator."]},{"cell_type":"code","metadata":{"id":"QkgCzWJ_UG2V"},"source":["model.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l1SIlxSVU2TY"},"source":["To check the number of neighbors we need to acces the \"clf\" step from the pipeline and then access its `n_neighbors` attribute:"]},{"cell_type":"code","metadata":{"id":"D319uy5pUbJu"},"source":["print(\"Number of neighbors (k):\", model[\"clf\"].n_neighbors)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nghV9JeXHmV_"},"source":["Transform the data, and apply `predict` with the final estimator:\n","\n","A `model.predict` call applies a `transform` to each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls `predict` method. This is only valid if the final estimator implements a `predict` method."]},{"cell_type":"code","metadata":{"id":"SxZEUVLWWH48"},"source":["pred_train = model.predict(X_train)\n","pred_test = model.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-j4sK2Q7I6Te"},"source":["We can now compute and plot the classification report with precision, recall, f-score per class and global accuracy"]},{"cell_type":"code","metadata":{"id":"OLW2KASzVQiu"},"source":["print(\"TRAINING\\n\" + classification_report(y_train, pred_train))\n","print(\"\\nTESTING\\n\" + classification_report(y_test, pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwE-3N_IJFds"},"source":["We can also compute the training and test error and confusion matrices"]},{"cell_type":"code","metadata":{"id":"vLzlkbC3V1IP"},"source":["train_error = 1. - accuracy_score(y_train, pred_train)\n","train_cmat = confusion_matrix(y_train, pred_train)\n","test_error = 1. - accuracy_score(y_test, pred_test)\n","test_cmat = confusion_matrix(y_test, pred_test)\n","\n","print('train error: %f ' % train_error)\n","print('train confusion matrix:')\n","print(train_cmat)\n","print('test error: %f ' % test_error)\n","print('test confusion matrix:')\n","print(test_cmat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2IWdefs6LLHb"},"source":["We can visualize the confusion matrix with `ConfusionMatrixDisplay`. See details [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay)"]},{"cell_type":"code","metadata":{"id":"xiuSa3rWKeRG"},"source":["display = ConfusionMatrixDisplay(confusion_matrix=test_cmat,display_labels=model[\"clf\"].classes_)\n","fig, ax = plt.subplots(figsize=(8,8))\n","display.plot(ax=ax, values_format='')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5XNszuW9ZNB7"},"source":["### Plotting \"eigen-images\" (only for dimensionality reduction)"]},{"cell_type":"code","metadata":{"id":"YMPz3POSWzpi"},"source":["if reduction > 0:\n","    if reduction == 1:\n","        components = model[\"reduce_dim\"].components_.reshape((n_components, 16, 16))\n","    else:\n","        components = model[\"reduce_dim\"].scalings_.T.reshape((n_components, 16, 16))\n","\n","    plt.figure(figsize=(1.8 * 2, 2.4 * 5))\n","    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n","    for i in range(min(n_components, 10)):\n","        plt.subplot(5, 2, i + 1)\n","        plt.imshow(components[i].reshape((16, 16)), cmap=plt.cm.gray)\n","        plt.xticks(())\n","        plt.yticks(())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eT8NBuUaazn9"},"source":["### Plotting reconstructed images"]},{"cell_type":"code","metadata":{"id":"0DQjaUrTa19F"},"source":["def plot_reconstructed_image(sample):\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","    ax1.imshow(sample.reshape((16, 16)), cmap=plt.cm.gray)\n","    ax1.set_title(\"Original\")\n","    sample_transformed = model[:2].transform(sample.reshape(1, -1))\n","    if reduction > 0:\n","        if reduction == 1:\n","            sample_reconstructed = model[1].inverse_transform(sample_transformed)\n","        else:\n","            sample_reconstructed = np.dot(sample_transformed, pinv(model[\"reduce_dim\"].scalings_))\n","    else:\n","        sample_transformed = sample\n","    ax2.imshow(sample_reconstructed.reshape((16, 16)), cmap=plt.cm.gray)\n","    ax2.set_title(\"Reconstructed\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTmyl3oakO8t"},"source":["Try the function using different samples."]},{"cell_type":"code","metadata":{"id":"N1am31D-cKGt"},"source":["if reduction > 0:\n","  plot_reconstructed_image(X_train[10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9VzsfJAq7GVB"},"source":["## Finding the best 'k' in kNN\n","\n","When evaluating different hyperparameters for estimators, such as the 'k' in kNN, we can't just train the model on the training set for different values of 'k', and evaluate them on the test set. There is a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n","\n","However, by partitioning the available data into three sets (training, validation and test), we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n","\n","In the following lab we will see different **cross-validation strategies** for dealing with this problem.\n","\n","In this lab we will use the simplest strategy, using a fixed single partition into train, validation and test subsets. We will train the model for different values of 'k' on the training set, and select the value of 'k' that achieves the best performance on the validation set.\n","\n"]},{"cell_type":"markdown","source":["We will use an approach named GridSearchCV, combined with the use of Pipelines (since we need to perform feature scaling and dimensionality reduction before training the model)"],"metadata":{"id":"p35-5JIM3QCX"}},{"cell_type":"markdown","metadata":{"id":"bmBovDZBBFiR"},"source":["### Using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n","\n","Two generic approaches to parameter search are provided in scikit-learn: for given values, `GridSearchCV` exhaustively considers all parameter combinations, while `RandomizedSearchCV` can sample a given number of candidates from a parameter space with a specified distribution.\n","\n","The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. The GridSearchCV instance implements the usual estimator API: when “fitting” it on a dataset, all the possible combinations of parameter values are evaluated and the best combination is retained. A GridSeachCV object internally iterates over a parameter grid and computes cross-validated scores for each hyper-parameter set.\n","\n","In this case we are only validating one hyper-parameter (`n_neighbors`), but we could validate multiple parameters from any step of our pipeline.\n"]},{"cell_type":"markdown","metadata":{"id":"vkmweXLdnH6W"},"source":["Next we create a GridSearchCV object. It has several parameters:\n","\n","* The `estimator` parameter, thas is the model we are using (in this case \"model\")\n","\n","* The `param_grid` parameter requires a list of parameters and the range of values for each parameter of the specified estimator. A list of values to choose from should be given to each hyperparameter of the model\n","\n","* A cross validation process `cv` that is performed in order to determine the hyperparameter value set which provides the best accuracy levels.\n","\n","Here, as we mentioned before, we will use a single split of the training data into training and validation sets, so  the `cv` will be `StratifiedShuffleSplit` with a single split. The training subset will contain 75% of the training data. StratifiedShuffleSplit  returns stratified splits, i.e creates splits by preserving the same percentage for each target class as in the complete set.\n","\n","For each value of the hyperparameter (or each combination of hyperparameters if there are more than one), the model will be trained on the training subset. It will return the best set of hyperparameters (evaluated on the validation set), and the model trained on these parameters.\n","\n","To understand the parameter grid syntax for pipelines, see [here](https://scikit-learn.org/stable/modules/compose.html#nested-parameters)."]},{"cell_type":"markdown","source":["We will define a new pipeline, `model2`. Previously, you can choose the reduction type (no reduction, PCA, MDA)\n","\n"],"metadata":{"id":"NRtyU35h4mrq"}},{"cell_type":"code","source":["reduction = int(input(\"Choose reduction type.\\nNo reduction (0), PCA (1), MDA(2): \"))\n","if reduction > 0:\n","    n_components = int(input(\"Choose dimenstionality: \"))\n","    if reduction == 1:\n","        dim_reducer = PCA(n_components=n_components)\n","    elif reduction == 2:\n","        dim_reducer = LinearDiscriminantAnalysis(n_components=n_components)\n","    else:\n","        dim_reducer = None\n","elif reduction == 0:\n","    dim_reducer = \"passthrough\"\n","else:\n","    dim_reducer = None\n","\n","if not dim_reducer:\n","    print(\"Please choose one of the valid options (run this cell again).\")"],"metadata":{"id":"mIxLSE3h4ymW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2 = Pipeline([\n","    ('center', StandardScaler(with_mean=True, with_std=False)),\n","    ('reduce_dim', dim_reducer),\n","    ('clf', KNeighborsClassifier())\n","])"],"metadata":{"id":"YVaTgQr711Ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k38nk9R2vP27"},"source":["# Here you can put the values of \"k\" to try.\n","\n","Ks = [1, 2, 3, 4, 5, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grid_search_cv = GridSearchCV(estimator = model2,\n","                              param_grid={\"clf__n_neighbors\": Ks},\n","                              cv=StratifiedShuffleSplit(n_splits=1, train_size=0.75,random_state=1), return_train_score=True)\n","grid_search_cv.fit(X_train, y_train)"],"metadata":{"id":"7emTo_Vc0skX"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FykZCrDSB74e"},"source":["plt.plot(grid_search_cv.cv_results_[\"param_clf__n_neighbors\"].data, grid_search_cv.cv_results_[\"mean_train_score\"], label=\"train\")\n","plt.plot(grid_search_cv.cv_results_[\"param_clf__n_neighbors\"].data, grid_search_cv.cv_results_[\"mean_test_score\"], label=\"val\")\n","plt.legend()\n","plt.title(\"Accuracy\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see the parameter setting that gave the best results on the hold out data(`best_params_`):\n","\n"],"metadata":{"id":"GQ4ZWxjc8EgF"}},{"cell_type":"code","source":["print(grid_search_cv.best_params_)"],"metadata":{"id":"rEM9Lob078at"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtgIB7yGM-oZ"},"source":["Calling `predict` on a fitted GridSearchCV object will make predictions using **the best set of hyper-parameters**."]},{"cell_type":"code","metadata":{"id":"PzjJPYKaM9RX"},"source":["pred_test = grid_search_cv.predict(X_test)\n","\n","test_error = 1. - accuracy_score(y_test, pred_test)\n","test_cmat = confusion_matrix(y_test, pred_test)\n","\n","print('test error: %f ' % test_error)\n","print('test confusion matrix:')\n","print(test_cmat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nbz47N4KEsco"},"source":["## Parzen windows with cross validation\n","Scikit-learn does not provide a classifer class for the Parzen windows method. We will implement it ourselves following the predictor interface by means of the `KernelDensity` class.\n"]},{"cell_type":"code","metadata":{"id":"IuL6OENLCxcm"},"source":["class ParzenClassifier(BaseEstimator, ClassifierMixin):\n","    densities = {}  # Dictionary containing density estimation for each class.\n","\n","    def __init__(self, bandwidth=1.0, kernel=\"gaussian\"):\n","        self.bandwidth = bandwidth\n","        self.kernel = kernel\n","\n","    def fit(self, X, y):\n","        self.classes_ = unique_labels(y)\n","        # Fit kernel density estimator (Parzen windows) for each class.\n","        for c in self.classes_:\n","            n = (y_train == c).sum()\n","            hn = self.bandwidth / np.sqrt(n)\n","            self.densities[c] = KernelDensity(bandwidth=hn, kernel=self.kernel).fit(X[y == c])\n","        return self\n","\n","    def predict(self, X):\n","        y_pred = np.empty(X.shape[0])\n","        for idx, x in enumerate(X):\n","            # Estimate log-probability of sample for each class.\n","            log_probs = {c: self.densities[c].score_samples(x.reshape(1, -1))\n","                         for c in self.classes_}\n","            y_pred[idx] = max(log_probs, key=log_probs.get)\n","\n","        return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CbMzngejj3_g"},"source":["Use the previous class (`ParzenClassifier`) and add the necesary code to\n","1. Create a pipeline `model_parzen` for the Parzen classifier, which (1)\n","standardizes data using `StandardScaler`, (2) reduces dimensionality and (3) applies the Parzen classifier.\n","2. Perform hyperparmeter search using `GridSearchCV` for the windows' bandwith h, using a single shuffled stratified random split.\n","4. Plot the mean score on the train and validation folds for each value of the hyperparameter\n","3. Using the best model, compute and display the test error and confusion matrix"]},{"cell_type":"code","metadata":{"id":"Y-Lp3K38vkg4"},"source":["# Here you can put the values of \"h\" to try.\n","Hs = [1, 10, 20]"],"execution_count":null,"outputs":[]}]}