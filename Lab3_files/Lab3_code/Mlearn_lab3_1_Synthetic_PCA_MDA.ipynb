{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mlearn_lab3_1_Synthetic_PCA_MDA.ipynb","provenance":[{"file_id":"1fc2EKWXDu2G4y52ERfv6x2m2Wm6I9G5e","timestamp":1602319233371}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RWmvERRXkgzE"},"source":["# **Machine Learning from Data**\n","\n","## Lab 3: Feature selection using MDA\n","\n","\n","2021 - Veronica Vilaplana - [GPI @ IDEAI](https://imatge.upc.edu/web/) Research group\n","\n","-----------------\n"]},{"cell_type":"markdown","metadata":{"id":"mIEmPcEhkjcq"},"source":["##Part1: A synthetic Gaussian dataset\n","##Feature selection by dimensionality reduction. PCA vs MDA"]},{"cell_type":"code","metadata":{"id":"k7Lj8Q1K32mj"},"source":["import pandas as pd             #import pandas with the alias pd\n","import numpy as np              #import numpy with the alias np\n","import seaborn as sns           #import seaborn with the alias sns\n","import scipy.stats as ss\n","import matplotlib.pyplot as plt #import matplotlib.pyplot with the alias plt\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_curve\n","from sklearn import metrics\n","from sklearn.decomposition import PCA\n","\n","\n","from numpy.random import default_rng\n","# initialize a random seed such that every execution will raise same random sequences of results\n","rng = default_rng(seed=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cqdog3954UFz"},"source":["###Dataset mean and cov\n","Definition of class means and covariance matrices (based on a given SNR)\n"]},{"cell_type":"code","metadata":{"id":"TzRFvdbj4Ccz"},"source":["SNR = 10\n","\n","# Design parameters\n","dist = 1\n","n_classes = 3\n","n_samples = 500\n","n_feat = 3\n","\n","# M_Means matrix containing class means\n","M_Means = dist * np.array([[1, 1, -1],[-1, 0, -1], [-1, -1, 0]]) \n","\n","#Energy computation\n","energy = 0\n","for i in range(0,n_classes):\n","  energy = energy + np.dot(M_Means[i],M_Means[i])\n","energy = energy / n_classes\n","\n","# Noise variance computation\n","SNR = 10 ** (SNR/10)\n","sig = energy / SNR\n","sig = sig / n_feat\n","\n","M_covar = np.zeros(shape=(n_classes,n_feat,n_feat))\n","D = sig * np.array([[1, 1, 1],[0.1, 0.3, 0.6], [2, 0.01, 0.99]])\n","\n","for i in range(0,n_classes):\n","  H = rng.normal(size=(n_feat,n_feat))\n","  Heval, Hevec = np.linalg.eig(np.matmul(H,H.T))\n","  M_covar[i,:,:] = np.matmul(Hevec.T, np.matmul(np.diag(D[i,:]), Hevec))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hz57l4mW4NgR"},"source":["### Generate Gaussain training and test datasets"]},{"cell_type":"code","metadata":{"id":"KCjV-FgM4dOO"},"source":["# Training dataset\n","X_train = np.empty((0,n_feat), float)\n","y_train = np.empty((0), int)\n","for i in range(0,n_classes):\n","  Xdata = rng.multivariate_normal(mean=M_Means[i,:], cov= M_covar[i,:,:], size= n_samples)\n","  ylab = i * np.ones((n_samples),dtype=int)\n","  X_train = np.append(X_train, Xdata, axis=0)\n","  y_train = np.append(y_train, ylab, axis=0)\n","\n","# shuffle data (xdata and labels, same order)\n","rp = rng.permutation(len(X_train))\n","X_train = X_train[rp]\n","y_train  = y_train[rp] \n","\n","# Test dataset\n","X_test = np.empty((0,n_feat), float)\n","y_test = np.empty((0), int)\n","for i in range(0,n_classes):\n","  Xdata = rng.multivariate_normal(mean=M_Means[i,:], cov= M_covar[i,:,:], size= n_samples)\n","  ylab = i * np.ones((n_samples),dtype=int)\n","  X_test = np.append(X_test, Xdata, axis=0)\n","  y_test = np.append(y_test, ylab, axis=0)\n","\n","# shuffle data (xdata and labels, same order)\n","rp = rng.permutation(len(X_test))\n","X_test = X_test[rp]\n","y_test  = y_test[rp] \n","\n","# keep a copy\n","XX_train = X_train\n","XX_test = X_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bk_9N3NiiY2E"},"source":["###Show 3D and 2D a scatter plots"]},{"cell_type":"code","metadata":{"id":"kan1H1gS4pBC"},"source":["# 3D scatter plot (not interactive in colab!)\n","\n","fig = plt.figure(figsize = (10,7))\n","ax = plt.axes(projection =\"3d\") \n","col = ['tab:blue','tab:orange', 'tab:green']\n","for idclass in range(0, n_classes):\n","  idx = y_train==idclass\n","  ax.scatter3D(X_train[idx,0], X_train[idx,1], X_train[idx,2], color = col[idclass], label='class %d' %idclass,alpha=0.5); \n"," \n","plt.title(\"simple 3D scatter plot\") \n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BkJUSG3EAq9s"},"source":["# scatter plot, original data\n","# create dataframex\n","Xt = pd.DataFrame(X_train,columns=['feat1','feat2','feat3'])\n","yt = pd.DataFrame(y_train,columns=['class'])\n","Xyt = Xt.join(yt)\n","#check dataframe\n","Xyt.head()\n","sns.pairplot(Xyt,hue='class',palette='deep');\n","plt.draw()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lVSWSnBY4rnZ"},"source":["###Define LDA and QDA Classifiers\n","We define functions to train and test LDA and QDA classifiers"]},{"cell_type":"code","metadata":{"id":"Pi1QFVM84sFJ"},"source":["def lda_classifier(X_train, y_train, X_test, y_test):\n","\n","  # Linear Discriminant Analysis\n","  lda = LinearDiscriminantAnalysis(solver=\"svd\",store_covariance=True)\n","  ldamodel = lda.fit(X_train, y_train)\n","  y_tpred_lda = ldamodel.predict(X_train)\n","  y_testpred_lda = ldamodel.predict(X_test)\n","\n","  lda_train_error = 1. - accuracy_score(y_train,y_tpred_lda)\n","  lda_train_cmat = metrics.confusion_matrix(y_train,y_tpred_lda)                                         \n","\n","  lda_test_error = 1. - accuracy_score(y_test,y_testpred_lda)\n","  lda_test_cmat = metrics.confusion_matrix(y_test,y_testpred_lda)\n","\n","  lda_error = np.array([lda_train_error, lda_test_error])\n","  lda_cmat  = np.array([lda_train_cmat, lda_test_cmat])\n","\n","  return lda, lda_error, lda_cmat\n","\n","def qda_classifier(X_train, y_train, X_test, y_test):\n","  # Quadratic Discriminant Analysis\n","  qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n","  qdamodel = qda.fit(X_train, y_train)\n","  y_tpred_qda = qdamodel.predict(X_train)\n","  y_testpred_qda = qdamodel.predict(X_test)\n","\n","  qda_train_error = 1. - accuracy_score(y_train,y_tpred_qda)\n","  qda_train_cmat = metrics.confusion_matrix(y_train,y_tpred_qda)\n","\n","  qda_test_error = 1. - accuracy_score(y_test,y_testpred_qda)\n","  qda_test_cmat = metrics.confusion_matrix(y_test,y_testpred_qda)\n","\n","  qda_error = np.array([qda_train_error, qda_test_error])\n","  qda_cmat  = np.array([qda_train_cmat, qda_test_cmat])\n","\n","  return qda, qda_error, qda_cmat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19Ni74v1oa41"},"source":["###Train and test LDA and QDA for 3D data\n","First, we train and test the classifiers using the original 3D data"]},{"cell_type":"code","metadata":{"id":"OMw1ta3poaHA"},"source":["# LDA 3D\n","lda, lda_error, lda_cmat = lda_classifier(X_train,y_train,X_test,y_test)\n","print('LDA train error: %f ' %lda_error[0])\n","print('LDA train confusion matrix:')\n","print(lda_cmat[0])\n","print('LDA test error: %f ' %lda_error[1] )\n","print('LDA test confusion matrix:')\n","print(lda_cmat[1])\n","\n","# QDA 3D\n","qda, qda_error, qda_cmat = qda_classifier(X_train,y_train,X_test,y_test)\n","print('QDA train error: %f ' %qda_error[0])\n","print('QDA train confusion matrix:')\n","print(qda_cmat[0])\n","print('QDA test error: %f ' %qda_error[1] )\n","print('QDA test confusion matrix:')\n","print(qda_cmat[1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kbUqBo1CFkaF"},"source":["###PCA: Project to 2 dim and classify with LDA and QDA\n","Then, we project the samples to 2 dim using PCA, train and test."]},{"cell_type":"code","metadata":{"id":"s6g9gmpjFoGi"},"source":["# PCA 2D\n","pca = PCA(n_components=2)\n","pca.fit(X_train)\n","X_train_pca2 = pca.transform(X_train)\n","X_test_pca2  = pca.transform(X_test)\n","\n","# Percentage of variance explained for each components\n","print('PCA PROJECTION TO 2D')\n","print('explained variance ratio (first two components): %s'\n","      % str(pca.explained_variance_ratio_))\n","\n","# Train and test LDA and QDA classifiers\n","# LDA 2D\n","lda, lda_error, lda_cmat = lda_classifier(X_train_pca2,y_train,X_test_pca2,y_test)\n","print('LDA train error: %f ' %lda_error[0])\n","print('LDA train confusion matrix:')\n","print(lda_cmat[0])\n","print('LDA test error: %f ' %lda_error[1] )\n","print('LDA test confusion matrix:')\n","print(lda_cmat[1])\n","\n","# QDA 2D\n","qda, qda_error, qda_cmat = qda_classifier(X_train_pca2,y_train,X_test_pca2,y_test)\n","print('QDA train error: %f ' %qda_error[0])\n","print('QDA train confusion matrix:')\n","print(qda_cmat[0])\n","print('QDA test error: %f ' %qda_error[1] )\n","print('QDA test confusion matrix:')\n","print(qda_cmat[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ePBvLWugKCmM"},"source":["###PCA 2D: then plots, scatter and boundaries"]},{"cell_type":"code","metadata":{"id":"RAH7icEGKOAr"},"source":["X = X_train_pca2\n","y = y_train\n","\n","# For the lineal model\n","h = .01 # step size in the mesh\n","colors = ['r','g','b']\n","classes = ['1', '2', '3']\n","\n","# create a mesh to plot in\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                     np.arange(y_min, y_max, h))\n","\n","fig, ax = plt.subplots(figsize=(14,7),nrows=1, ncols=2)\n","\n","Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n","# Put the result into a color plot\n","Z = Z.reshape(xx.shape)\n","\n","for idc, nc in enumerate(classes):\n","  idx = y== (idc)\n","  ax[0].scatter(X[idx,0], X[idx,1], color = colors[idc], label='class %d' %idc,alpha=0.7); \n"," \n","ax[0].contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.2)\n","\n","ax[0].set_xlim(xx.min(), xx.max())\n","ax[0].set_ylim(yy.min(), yy.max())\n","ax[0].set_xticks(())\n","ax[0].set_yticks(())\n","ax[0].set_title('PCA 2D - LDA boundaries')\n","\n","Z = qda.predict(np.c_[xx.ravel(), yy.ravel()])\n","# Put the result into a color plot\n","Z = Z.reshape(xx.shape)\n","\n","# Plot also the training points\n","for idc, nc in enumerate(classes):\n","  idx = y== (idc)\n","  ax[1].scatter(X[idx,0], X[idx,1], color = colors[idc], label='class %d' %idc,alpha=0.7);\n","\n","ax[1].contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.2)\n","\n","ax[1].set_xlim(xx.min(), xx.max())\n","ax[1].set_ylim(yy.min(), yy.max())\n","ax[1].set_xticks(())\n","ax[1].set_yticks(())\n","ax[1].set_title('PCA 2D - QDA boundaries')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VZJjh67wQqj7"},"source":["###PCA: Project to 1 dim and classify with LDA and QDA\n","Finally we project the samples to the first principal component"]},{"cell_type":"code","metadata":{"id":"PUSWCzDMRCJc"},"source":["# PCA 1D\n","pca = PCA(n_components=1)\n","pca.fit(X_train)\n","X_train_pca1 = pca.transform(X_train)\n","X_test_pca1  = pca.transform(X_test)\n","\n","# Percentage of variance explained for each components\n","print('PCA PROJECTION TO 1D')\n","print('explained variance ratio (first two components): %s'\n","      % str(pca.explained_variance_ratio_))\n","\n","# Train and test LDA and QDA classifiers\n","# LDA 1D\n","lda, lda_error, lda_cmat = lda_classifier(X_train_pca1,y_train,X_test_pca1,y_test)\n","print('LDA train error: %f ' %lda_error[0])\n","print('LDA train confusion matrix:')\n","print(lda_cmat[0])\n","print('LDA test error: %f ' %lda_error[1] )\n","print('LDA test confusion matrix:')\n","print(lda_cmat[1])\n","\n","# QDA 1D\n","qda, qda_error, qda_cmat = qda_classifier(X_train_pca1,y_train,X_test_pca1,y_test)\n","print('QDA train error: %f ' %qda_error[0])\n","print('QDA train confusion matrix:')\n","print(qda_cmat[0])\n","print('QDA test error: %f ' %qda_error[1] )\n","print('QDA test confusion matrix:')\n","print(qda_cmat[1])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nd37Bjp7RYb5"},"source":["###PCA 1D: scatter plot"]},{"cell_type":"code","metadata":{"id":"Y2_hqh26RaUD"},"source":["X = X_train_pca1\n","y = y_train\n","\n","# For the lineal model\n","colors = ['r','g','b']\n","classes = ['1', '2', '3']\n","\n","# create a mesh to plot in\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                     np.arange(y_min, y_max, h))\n","\n","fig, ax = plt.subplots(figsize=(7,7))\n","\n","for idc, nc in enumerate(classes):\n","  idx = y== (idc)\n","  ax.scatter(X[idx,0], X[idx,0], color = colors[idc], label='class %d' %idc,alpha=0.5); \n","\n","ax.set_xlim(xx.min(), xx.max())\n","ax.set_ylim(yy.min(), yy.max())\n","ax.set_xticks(())\n","ax.set_yticks(())\n","ax.set_title('PCA 1D')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ZdfxWkxToci"},"source":["###MDA (LDA in ScikitLearn): Project to 2 dim and classify with LDA and QDA\n"]},{"cell_type":"code","metadata":{"id":"bph_JhBTTvMm"},"source":["# MDA 2D\n","mda = LinearDiscriminantAnalysis(n_components=2)\n","mda.fit(X_train, y_train)\n","X_train_mda2 = mda.transform(X_train)\n","X_test_mda2  = mda.transform(X_test)\n","\n","# Percentage of variance explained for each components\n","print('MDA PROJECTION TO 2D')\n","print('explained variance ratio (first two components): %s'\n","      % str(mda.explained_variance_ratio_))\n","\n","# Train and test LDA and QDA classifiers\n","# LDA 2D\n","lda, lda_error, lda_cmat = lda_classifier(X_train_mda2,y_train,X_test_mda2,y_test)\n","print('LDA train error: %f ' %lda_error[0])\n","print('LDA train confusion matrix:')\n","print(lda_cmat[0])\n","print('LDA test error: %f ' %lda_error[1] )\n","print('LDA test confusion matrix:')\n","print(lda_cmat[1])\n","\n","# QDA 2D\n","qda, qda_error, qda_cmat = qda_classifier(X_train_mda2,y_train,X_test_mda2,y_test)\n","print('QDA train error: %f ' %qda_error[0])\n","print('QDA train confusion matrix:')\n","print(qda_cmat[0])\n","print('QDA test error: %f ' %qda_error[1] )\n","print('QDA test confusion matrix:')\n","print(qda_cmat[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2bHROubWT5J"},"source":["###MDA 2D: then plots, scatter and boundaries"]},{"cell_type":"code","metadata":{"id":"YofMuqTuV4B8"},"source":["#plot 2D \n","\n","X = X_train_mda2\n","y = y_train\n","\n","# For the lineal model\n","h = .01 # step size in the mesh\n","colors = ['r','g','b']\n","classes = ['1', '2', '3']\n","\n","# create a mesh to plot in\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                     np.arange(y_min, y_max, h))\n","\n","fig, ax = plt.subplots(figsize=(14,7),nrows=1, ncols=2)\n","\n","Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n","# Put the result into a color plot\n","Z = Z.reshape(xx.shape)\n","\n","for idc, nc in enumerate(classes):\n","  idx = y== (idc)\n","  ax[0].scatter(X[idx,0], X[idx,1], color = colors[idc], label='class %d' %idc,alpha=0.7); \n"," \n","ax[0].contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.2)\n","\n","ax[0].set_xlim(xx.min(), xx.max())\n","ax[0].set_ylim(yy.min(), yy.max())\n","ax[0].set_xticks(())\n","ax[0].set_yticks(())\n","ax[0].set_title('MDA 2D - LDA boundaries')\n","\n","Z = qda.predict(np.c_[xx.ravel(), yy.ravel()])\n","# Put the result into a color plot\n","Z = Z.reshape(xx.shape)\n","\n","# Plot also the training points\n","for idc, nc in enumerate(classes):\n","  idx = y== (idc)\n","  ax[1].scatter(X[idx,0], X[idx,1], color = colors[idc], label='class %d' %idc,alpha=0.7);\n","\n","ax[1].contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.2)\n","\n","ax[1].set_xlim(xx.min(), xx.max())\n","ax[1].set_ylim(yy.min(), yy.max())\n","ax[1].set_xticks(())\n","ax[1].set_yticks(())\n","ax[1].set_title('MDA 2D - QDA boundaries')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vGwRoHObWaKA"},"source":["###MDA (LDA in ScikitLearn): Now project to 1 dim and classify with LDA and QDA\n"]},{"cell_type":"code","metadata":{"id":"TvQCP60WW4l9"},"source":["# MDA 1D\n","mda = LinearDiscriminantAnalysis(n_components=1)\n","mda.fit(X_train, y_train)\n","X_train_mda1 = mda.transform(X_train)\n","X_test_mda1  = mda.transform(X_test)\n","\n","# Percentage of variance explained for each components\n","print('MDA PROJECTION TO 1D')\n","print('explained variance ratio (first two components): %s'\n","      % str(mda.explained_variance_ratio_))\n","\n","# Train and test LDA and QDA classifiers\n","# LDA 1D\n","lda, lda_error, lda_cmat = lda_classifier(X_train_mda1,y_train,X_test_mda1,y_test)\n","print('LDA train error: %f ' %lda_error[0])\n","print('LDA train confusion matrix:')\n","print(lda_cmat[0])\n","print('LDA test error: %f ' %lda_error[1] )\n","print('LDA test confusion matrix:')\n","print(lda_cmat[1])\n","\n","# QDA 1D\n","qda, qda_error, qda_cmat = qda_classifier(X_train_mda1,y_train,X_test_mda1,y_test)\n","print('QDA train error: %f ' %qda_error[0])\n","print('QDA train confusion matrix:')\n","print(qda_cmat[0])\n","print('QDA test error: %f ' %qda_error[1] )\n","print('QDA test confusion matrix:')\n","print(qda_cmat[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"04oJ49-aWjfb"},"source":["###MDA: plot 1D, LDA and QDA"]},{"cell_type":"code","metadata":{"id":"p9Zq0CFVVj5c"},"source":["#MDA 1D\n","X = X_train_mda1\n","y = y_train\n","\n","# For the lineal model\n","colors = ['g','b','r']\n","classes = ['1', '2', '3']\n","\n","# create a mesh to plot in\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                     np.arange(y_min, y_max, h))\n","\n","fig, ax = plt.subplots(figsize=(7,7))\n","\n","for idc, nc in enumerate(classes):\n","  idx = y== (idc)\n","  ax.scatter(X[idx,0], X[idx,0], color = colors[idc], label='class %d' %idc,alpha=0.5); \n","\n","ax.set_xlim(xx.min(), xx.max())\n","ax.set_ylim(yy.min(), yy.max())\n","ax.set_xticks(())\n","ax.set_yticks(())\n","ax.set_title('MDA 1D')\n","\n","plt.show()"],"execution_count":null,"outputs":[]}]}