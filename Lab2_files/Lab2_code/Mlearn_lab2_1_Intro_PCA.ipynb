{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mlearn_lab2_1_Intro_PCA.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNq3G4wxXZMFnYXITGqy7Ws"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"00RAJfMXvDvQ"},"source":["# **Machine Learning from Data**\n","\n","## Lab 2: Feature selection using PCA\n","\n","\n","2021 - Veronica Vilaplana - [GPI @ IDEAI](https://imatge.upc.edu/web/) Research group\n","\n","-----------------"]},{"cell_type":"markdown","metadata":{"id":"FMt-R-hcQcru"},"source":["#Part1: PCA and Dimensionality reduction\n","\n","Based on\n","* “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition,\n","by Aurélien Géron (O’Reilly). Copyright 2019 Aurélien Géron, 978-1-492-\n","03264-9.”\n","* \"Python Data Science Handbook by Jake VanderPlas (O'Reilly).\""]},{"cell_type":"markdown","metadata":{"id":"N4dSQLfMbZHQ"},"source":["Many machine learning problems involve  thousands of features for each training instance. These features make training extremely slow, and cand also make it harder to find a good solution. This problem is often referred as *the curse of dimensionality*.\n","\n","In many real-world problems it is often possible to reduce the number of features considerably, turning an intractable problem into a tractable one.\n","\n","In addition to speeding up training, dimensionality reduction is useful for data visualization: reducing the number of dimensions to two or three makes it possible to plot a condensed view of a high dimensional set and visually detecting patterns or clusters.\n","\n","In this exercise we will analyze one approach for dimensionality reduction: projection. It is based on the observation that in many problems, instances are not spread out uniformly across all dimensions. Features may be constant or correlated, and as a result, samples lies within or close to a much lower dimensional subspace of the original high dimensional space. \n","\n","Principal Component Analysis is one of the most popular dimensionality reduction techniques, which identifies the hyperplane that lies closest to the data and then projects the data on it.\n","\n","There are other problems where the subspace where samples lie may twist and turn. In that case, projection methods may not be adequate, and other methods are used, methods that work by modeling the manifold where the samples lie (manifold learning approaches). This kind of problems and solutions are outside the scope of this lab."]},{"cell_type":"markdown","metadata":{"id":"EGxpmK0sk48N"},"source":["We start importing the necessary libraries"]},{"cell_type":"code","metadata":{"id":"ILs-14t8bgo0"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y1rdFUrTbfrH"},"source":["## Introducing Principal Component Analysis\n","\n","Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data.\n","\n","Consider the following 200 random points:"]},{"cell_type":"code","metadata":{"id":"S9Y4blkeb3UF"},"source":["rng = np.random.RandomState(1)\n","X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n","plt.scatter(X[:, 0], X[:, 1])\n","plt.axis('equal');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z9ngSOFib6NW"},"source":["There is a near linear relationship between the x and y variables. We want to learn about this relationship. In Principal Component Analysis this relationship is quantified by finding the *principal axes* in the data, and using these axes to describe the dataset.\n","\n","We use scikit-learn `PCA` estimator to compute it:"]},{"cell_type":"code","metadata":{"id":"Zj6v2tLkcZ9i"},"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2)\n","pca.fit(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a5zBBuANcfKW"},"source":["This fit learns from the data the \"components\" and the \"explained variance\"\n","\n","`components_` is a ndarray of shape (n_components, n_features), it contains the principal axes in the feature space, representing the directions of maximum variance in the data. The components are sorted by `explained_variance_`.\n","\n","`explained_variance_` is the amount of variance explained by each of the selected components. The variance estimation uses n_samples - 1 degrees of freedom.\n","\n","`explained_variance_ratio_` provides the percentage of variance explained by each of the selected components."]},{"cell_type":"code","metadata":{"id":"ie8_tGIQcnsz"},"source":["print(pca.components_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzTRhMlbcreA"},"source":["print(pca.explained_variance_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pca.explained_variance_ratio_)"],"metadata":{"id":"ujW7dz6sQuOc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The first dimension explains about 98% of the variance, while the second explains about 2%."],"metadata":{"id":"Z0-fgM0zQsH4"}},{"cell_type":"markdown","metadata":{"id":"I7WPImLlctlA"},"source":["We will visualize this as vectors over the input data. The \"components\" will define the direction of the vectors, and the \"explained variance\" will define the squared length of the vector:"]},{"cell_type":"code","metadata":{"id":"ZabScUtHc_sn"},"source":["def draw_vector(v0, v1, ax=None):\n","    ax = ax or plt.gca()\n","    arrowprops=dict(arrowstyle='->',\n","                    linewidth=2,\n","                    shrinkA=0, shrinkB=0, color='k')\n","    ax.annotate('', v1, v0, arrowprops=arrowprops)\n","\n","# plot data\n","plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n","for length, vector in zip(pca.explained_variance_, pca.components_):\n","    v = vector * 3 * np.sqrt(length)\n","    draw_vector(pca.mean_, pca.mean_ + v)\n","plt.axis('equal');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJgQJbtiemSd"},"source":["The vectors represent the *principal axes* of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the data. It is a measure of the variance of the data when projected onto that axis.\n","The projection of each data point onto the principal axes are the \"principal components\" of the data.\n"]},{"cell_type":"markdown","metadata":{"id":"LywZp5i_kLlo"},"source":["### PCA as dimensionality reduction\n","\n","Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n","\n","Following the previous example, we project the data to the first principal component."]},{"cell_type":"code","metadata":{"id":"EA17grsskcAL"},"source":["pca = PCA(n_components=1)\n","pca.fit(X)\n","X_pca = pca.transform(X)\n","print(\"original shape:   \", X.shape)\n","print(\"transformed shape:\", X_pca.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e5YiweDikgSE"},"source":["The transformed data has been reduced to a single dimension.\n","To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"]},{"cell_type":"code","metadata":{"id":"xXidBJxdknzt"},"source":["X_new = pca.inverse_transform(X_pca)\n","plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n","plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8, color='r')\n","plt.axis('equal');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mmtvcar2k-nx"},"source":["The light blue points are the original data, while the red points are the projected version. The information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.\n","The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n","\n","This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."]},{"cell_type":"markdown","metadata":{"id":"_BH5Xl7blPxf"},"source":["### PCA for visualization: Hand-written digits\n","\n","The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data.\n","\n","We start by loading the data:"]},{"cell_type":"code","metadata":{"id":"yJHn51iflVk3"},"source":["from sklearn.datasets import load_digits\n","digits = load_digits()\n","digits.data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VsWAh9-0qa1G"},"source":["The data consists of 8×8 pixel images, meaning that they are 64-dimensional. Let's visualize the first few data samples"]},{"cell_type":"code","metadata":{"id":"eTcQCGAqqgCt"},"source":["# set up the figure\n","fig = plt.figure(figsize=(6, 6))  # figure size in inches\n","fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n","\n","# plot the digits: each image is 8x8 pixels\n","for i in range(64):\n","    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n","    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n","    \n","    # label the image with the target value\n","    ax.text(0, 7, str(digits.target[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fDcvWALDlhIF"},"source":["To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:"]},{"cell_type":"code","metadata":{"id":"4ZjlcC69lloq"},"source":["pca = PCA(2)  # project from 64 to 2 dimensions\n","projected = pca.fit_transform(digits.data)\n","print(digits.data.shape)\n","print(projected.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7UpB6oalZ3c"},"source":["We can now plot the first two principal components of each point to learn about the data. The plot shows the projection of each data point along the directions with the largest variance. We will use different colors to represent the digits, so we can see if the projected samples corresponding to the same class (digit) are cluster in the plane.\n"]},{"cell_type":"code","metadata":{"id":"W9whNkLPlXyl"},"source":["plt.scatter(projected[:, 0], projected[:, 1],\n","            c=digits.target, edgecolor='none', alpha=0.5,\n","            cmap=plt.cm.get_cmap('jet', 10))\n","plt.xlabel('component 1')\n","plt.ylabel('component 2')\n","plt.colorbar();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6I9P46fGrN6w"},"source":["### What do the components mean?\n","\n","PCA can be thought of as a process of choosing optimal basis functions, such that adding together just the first few of them is enough to suitably reconstruct the bulk of the elements in the dataset.\n","The principal components, which act as the low-dimensional representation of the data, are simply the coefficients that multiply each of the elements in this series.\n","\n","We will illustrate this meaning in a following lab."]},{"cell_type":"markdown","metadata":{"id":"59aOgGmftSx8"},"source":["### Choosing the number of components\n","\n","An important part of using PCA in practice is the ability to estimate how many components are needed to describe the data.\n","This can be determined by looking at the cumulative *explained variance ratio* as a function of the number of components.\n","\n","For the digits dataset:"]},{"cell_type":"code","metadata":{"id":"do9RaWK_tQ1S"},"source":["pca = PCA().fit(digits.data)\n","plt.plot(np.cumsum(pca.explained_variance_ratio_))\n","plt.xlabel('number of components')\n","plt.ylabel('cumulative explained variance');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HgiFUDMOuc4V"},"source":["This curve quantifies how much of the total, 64-dimensional variance is contained within the first $N$ components.\n","For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.\n","\n","Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we'd need about 20 components to retain 90% of the variance.  Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations."]},{"cell_type":"markdown","source":["## t-SNE\n","\n","Finally, we will use tSNE to reduce the digit dataset down to two dimensions and plot the result. We will use different colors to represent the digits, so we can see if the projected samples corresponding to the same class (digit) are cluster in the plane."],"metadata":{"id":"fHg3Ud9iTwsr"}},{"cell_type":"code","source":["from sklearn.manifold import TSNE\n","\n","tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\",\n","            random_state=1)\n","X_reduced = tsne.fit_transform(digits.data)"],"metadata":{"id":"KwPnzGeqT00G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(13, 10))\n","plt.scatter(X_reduced[:, 0], X_reduced[:, 1],\n","            c=digits.target, edgecolor='none', alpha=0.5,\n","            cmap=plt.cm.get_cmap('jet', 10))\n","#plt.axis('off')\n","plt.colorbar();\n","plt.show()"],"metadata":{"id":"v9R5DSwiT3I2"},"execution_count":null,"outputs":[]}]}